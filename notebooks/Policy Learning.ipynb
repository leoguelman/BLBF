{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Learning Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lguelman/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from blbf.STBT import STBT\n",
    "from blbf.PolicyLearning import EvaluationMetrics, RewardPredictor, OutcomeWeightedLearning, CounterfactualRiskMinimization, CounterfactualRiskMinimizationCV, LinearModel, NonLinearModel\n",
    "import blbf.DataReader as DataReader\n",
    "import blbf.utils as utils\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X, y = DataReader.get_data(dataset= 'glass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Supervised-to-Bandit Conversion \n",
    "\n",
    "Performs Supervised to Bandit Conversion for classification datasets. This conversion is generally used to test the limits of counterfactual learning in a well-controlled environment. See [1-3]. \n",
    "\n",
    "Here, we take a supervised dataset with features $x$ and labeled classes $y$, and simulate a bandit feedback data set from a logging policy. Basically, this involves: (i) simulating a stochastic logging policy, which may be  uniform (`logging_type='uniform'`), or given as a function of covariates (`logging_type = 'biased'`), (ii) when the logging policy for a given observation equals the optimal policy (true label), a positive reward is observed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = STBT(train_frac= 0.5, logging_type='biased').generate_batch(X, y, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skyline \n",
    "\n",
    "Best possible error rate, assuming we have full feedback (this can only be tested from the simulation as in practice as we have bandit feedback)x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skyline Error: 0.30841121495327106\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(multi_class='multinomial', max_iter=2000).fit(data.X_train, data.y_train)\n",
    "optimal_policy = clf.predict(data.X_test)\n",
    "print(\"Skyline Error:\", EvaluationMetrics.error_rate(optimal_policy, data.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reward Predictor (RP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Predictor Error: 0.7009345794392523\n"
     ]
    }
   ],
   "source": [
    "rp = RewardPredictor()\n",
    "rp.learn_policy(data, max_iter=1000)\n",
    "print(\"Reward Predictor Error:\", rp.error_rate(rp.est_best_policy, data.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Weighted Learning (OWL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWL-LR: 0.7289719626168225\n"
     ]
    }
   ],
   "source": [
    "owl = OutcomeWeightedLearning()\n",
    "owl.learn_policy(data, clf = 'LogisticRegressionCV', max_iter=1000)\n",
    "print(\"OWL-LR:\", owl.error_rate(owl.est_best_policy, data.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Risk Minimization (CRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: | Train Poem Loss: -0.10522\n",
      "Epoch 100: | Train Poem Loss: -0.25576\n",
      "Epoch 200: | Train Poem Loss: -0.39687\n",
      "Epoch 300: | Train Poem Loss: -0.49617\n",
      "Epoch 400: | Train Poem Loss: -0.54126\n",
      "Epoch 500: | Train Poem Loss: -0.56482\n",
      "Epoch 600: | Train Poem Loss: -0.58157\n",
      "Epoch 700: | Train Poem Loss: -0.59403\n",
      "Epoch 800: | Train Poem Loss: -0.60416\n",
      "Epoch 900: | Train Poem Loss: -0.61307\n",
      "Epoch 1000: | Train Poem Loss: -0.62120\n",
      "Epoch 1100: | Train Poem Loss: -0.62863\n",
      "Epoch 1200: | Train Poem Loss: -0.63533\n",
      "Epoch 1300: | Train Poem Loss: -0.64126\n",
      "Epoch 1400: | Train Poem Loss: -0.64643\n",
      "Epoch 1500: | Train Poem Loss: -0.65089\n",
      "Epoch 1600: | Train Poem Loss: -0.65471\n",
      "Epoch 1700: | Train Poem Loss: -0.65798\n",
      "Epoch 1800: | Train Poem Loss: -0.66077\n",
      "Epoch 1900: | Train Poem Loss: -0.66315\n",
      "CRM: 0.7009345794392523\n"
     ]
    }
   ],
   "source": [
    "crm = CounterfactualRiskMinimization(verbose=True, lambda_ = 1e-06)\n",
    "crm.learn_policy(model=LinearModel, data=data, epochs = 2000)\n",
    "print(\"CRM:\", crm.error_rate(crm.est_best_policy, data.y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params\n",
    "\n",
    "B = 10 # Number of simulations\n",
    "EPOCHS = 500\n",
    "LOGGING_TYPE = 'biased'\n",
    "MODEL = LinearModel\n",
    "LAMBDA = 1e-06\n",
    "DATASETS = ['ecoli', 'glass', 'lymphography', 'yeast', 'digits', 'breast-cancer', 'wine', 'letter-recognition']\n",
    "dat = list()\n",
    "skyline_error = list()\n",
    "randomized_error = list()\n",
    "reward_predictor_error = list()\n",
    "owl_lrcv_error = list()\n",
    "crm_error = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 0 - Dataset: ecoli\n",
      "Sample: 0 - Dataset: glass\n",
      "Sample: 0 - Dataset: lymphography\n",
      "Sample: 0 - Dataset: yeast\n",
      "Sample: 0 - Dataset: digits\n",
      "Sample: 0 - Dataset: breast-cancer\n",
      "Sample: 0 - Dataset: wine\n",
      "Sample: 0 - Dataset: letter-recognition\n"
     ]
    }
   ],
   "source": [
    "for s in DATASETS:\n",
    "    \n",
    "    X, y = DataReader.get_data(dataset=s)\n",
    "    \n",
    "    for b in range(B):\n",
    "        if (b % 10) == 0:\n",
    "            print(\"Sample: %d - Dataset: %s\" % (b, s))\n",
    "        \n",
    "        d = STBT(logging_type = LOGGING_TYPE).generate_batch(X, y, max_iter=1000)\n",
    "        dat.append(s)    \n",
    "       \n",
    "        skyline = LogisticRegression(multi_class='multinomial', max_iter=2000).fit(d.X_train, d.y_train)\n",
    "        optimal_policy = skyline.predict(d.X_test)\n",
    "        \n",
    "        rp = RewardPredictor().learn_policy(data=d, max_iter=1000)\n",
    "        erm_lrcv = OutcomeWeightedLearning().learn_policy(data=d, clf = 'LogisticRegressionCV', max_iter=1000)\n",
    "        crm = CounterfactualRiskMinimization(lambda_=LAMBDA).learn_policy(model=MODEL, data=d, epochs=EPOCHS)     \n",
    "        \n",
    "        skyline_error.append(EvaluationMetrics.error_rate(optimal_policy, d.y_test))\n",
    "        randomized_error.append(EvaluationMetrics.error_rate(d.y_test_logging, d.y_test))\n",
    "        reward_predictor_error.append(rp.error_rate(rp.est_best_policy, d.y_test))\n",
    "        owl_lrcv_error.append(erm_lrcv.error_rate(erm_lrcv.est_best_policy, d.y_test))\n",
    "        crm_error.append(crm.error_rate(crm.est_best_policy, d.y_test))\n",
    "           \n",
    "    \n",
    "res = pd.DataFrame.from_dict({'dataset':dat, 'skyline_error': skyline_error, 'randomized_error':randomized_error, 'reward_predictor_error':reward_predictor_error,\n",
    "                              'owl_lrcv_error':owl_lrcv_error, 'crm_error':crm_error})\n",
    "\n",
    "res_summary = res.groupby(['dataset'], as_index=False).agg({\n",
    "                            'skyline_error': ['mean','std'], \n",
    "                            'randomized_error': ['mean','std'], \n",
    "                            'reward_predictor_error': ['mean','std'],\n",
    "                            'owl_lrcv_error': ['mean','std'],\n",
    "                            'crm_error': ['mean','std']\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"2\" halign=\"left\">skyline_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">randomized_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">reward_predictor_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">owl_lrcv_error</th>\n",
       "      <th colspan=\"2\" halign=\"left\">crm_error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>breast-cancer</td>\n",
       "      <td>0.029474</td>\n",
       "      <td>0.006233</td>\n",
       "      <td>0.525614</td>\n",
       "      <td>0.227465</td>\n",
       "      <td>0.027719</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>0.170526</td>\n",
       "      <td>0.185646</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.143245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>digits</td>\n",
       "      <td>0.039933</td>\n",
       "      <td>0.007786</td>\n",
       "      <td>0.892102</td>\n",
       "      <td>0.044240</td>\n",
       "      <td>0.362291</td>\n",
       "      <td>0.082093</td>\n",
       "      <td>0.525250</td>\n",
       "      <td>0.091903</td>\n",
       "      <td>0.415239</td>\n",
       "      <td>0.112133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecoli</td>\n",
       "      <td>0.126786</td>\n",
       "      <td>0.015636</td>\n",
       "      <td>0.850595</td>\n",
       "      <td>0.099818</td>\n",
       "      <td>0.301786</td>\n",
       "      <td>0.070432</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.198541</td>\n",
       "      <td>0.344643</td>\n",
       "      <td>0.103649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>glass</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.047654</td>\n",
       "      <td>0.848598</td>\n",
       "      <td>0.076155</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.077193</td>\n",
       "      <td>0.603738</td>\n",
       "      <td>0.107664</td>\n",
       "      <td>0.651402</td>\n",
       "      <td>0.083015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>letter-recognition</td>\n",
       "      <td>0.229430</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>0.962720</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>0.736790</td>\n",
       "      <td>0.047752</td>\n",
       "      <td>0.713680</td>\n",
       "      <td>0.041299</td>\n",
       "      <td>0.739550</td>\n",
       "      <td>0.033839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lymphography</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.114533</td>\n",
       "      <td>0.274324</td>\n",
       "      <td>0.039295</td>\n",
       "      <td>0.327027</td>\n",
       "      <td>0.100683</td>\n",
       "      <td>0.414865</td>\n",
       "      <td>0.120541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wine</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.014793</td>\n",
       "      <td>0.744944</td>\n",
       "      <td>0.151215</td>\n",
       "      <td>0.070787</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.410112</td>\n",
       "      <td>0.228510</td>\n",
       "      <td>0.188764</td>\n",
       "      <td>0.151655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yeast</td>\n",
       "      <td>0.418329</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.899461</td>\n",
       "      <td>0.038157</td>\n",
       "      <td>0.523046</td>\n",
       "      <td>0.031120</td>\n",
       "      <td>0.623181</td>\n",
       "      <td>0.039180</td>\n",
       "      <td>0.558086</td>\n",
       "      <td>0.036266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataset skyline_error           randomized_error            \\\n",
       "                               mean       std             mean       std   \n",
       "0       breast-cancer      0.029474  0.006233         0.525614  0.227465   \n",
       "1              digits      0.039933  0.007786         0.892102  0.044240   \n",
       "2               ecoli      0.126786  0.015636         0.850595  0.099818   \n",
       "3               glass      0.411215  0.047654         0.848598  0.076155   \n",
       "4  letter-recognition      0.229430  0.003287         0.962720  0.010833   \n",
       "5        lymphography      0.216216  0.047244         0.750000  0.114533   \n",
       "6                wine      0.024719  0.014793         0.744944  0.151215   \n",
       "7               yeast      0.418329  0.016835         0.899461  0.038157   \n",
       "\n",
       "  reward_predictor_error           owl_lrcv_error           crm_error  \\\n",
       "                    mean       std           mean       std      mean   \n",
       "0               0.027719  0.009127       0.170526  0.185646  0.102807   \n",
       "1               0.362291  0.082093       0.525250  0.091903  0.415239   \n",
       "2               0.301786  0.070432       0.429167  0.198541  0.344643   \n",
       "3               0.579439  0.077193       0.603738  0.107664  0.651402   \n",
       "4               0.736790  0.047752       0.713680  0.041299  0.739550   \n",
       "5               0.274324  0.039295       0.327027  0.100683  0.414865   \n",
       "6               0.070787  0.048270       0.410112  0.228510  0.188764   \n",
       "7               0.523046  0.031120       0.623181  0.039180  0.558086   \n",
       "\n",
       "             \n",
       "        std  \n",
       "0  0.143245  \n",
       "1  0.112133  \n",
       "2  0.103649  \n",
       "3  0.083015  \n",
       "4  0.033839  \n",
       "5  0.120541  \n",
       "6  0.151655  \n",
       "7  0.036266  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
